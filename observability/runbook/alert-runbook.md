# Alert Runbook for OpenTelemetry Demo

This runbook provides step-by-step procedures for responding to alerts generated by the OpenTelemetry Demo observability stack.

## High Latency

### Alert Details
- **Alert Name**: `HighLatency` / `CriticalLatency`
- **Severity**: Warning / Critical
- **Threshold**: P95 latency > 500ms (warning) or > 2s (critical) for 5 minutes
- **Affected**: Services with high latency (frontend, checkout, product-catalog, cart, payment)

### What Causes This Alert
- Slow database queries
- Network latency issues
- Resource constraints (CPU/memory)
- Downstream service delays
- High traffic load
- Garbage collection pauses (Java services)
- Cold starts (serverless functions)

### Diagnosis Steps

1. **Identify the affected service:**
   ```bash
   # Check which service is experiencing high latency
   kubectl get pods -n otel-demo -o wide
   
   # View service logs
   kubectl logs -n otel-demo -l app.kubernetes.io/name=<service-name> --tail=100
   ```

2. **Check Prometheus metrics:**
   ```bash
   # Port-forward to Prometheus
   kubectl port-forward -n observability svc/observability-prometheus-kube-prom-prometheus 9090:9090
   
   # Query in Prometheus UI: http://localhost:9090
   # Query: histogram_quantile(0.95, sum(rate(otel_demo_request_duration_seconds_bucket[5m])) by (le, service_name))
   ```

3. **Check resource usage:**
   ```bash
   # Check CPU and memory usage
   kubectl top pods -n otel-demo
   
   # Check pod resource limits
   kubectl describe pod <pod-name> -n otel-demo | grep -A 5 "Limits"
   ```

4. **View traces in Jaeger:**
   ```bash
   # Port-forward to Jaeger
   kubectl port-forward -n otel-demo svc/jaeger-query 16686:16686
   
   # Open Jaeger UI: http://localhost:16686
   # Search for slow traces in the affected service
   ```

5. **Check CloudWatch Logs:**
   ```bash
   # Query CloudWatch Logs Insights for slow requests
   # Use the sample query from observability/cloudwatch/sample-query.md
   # Filter by latency_ms > 500
   ```

### Resolution Steps

1. **Scale up the service:**
   ```bash
   # Increase replica count
   kubectl scale deployment <service-name> --replicas=3 -n otel-demo
   
   # Or use HPA if configured
   kubectl get hpa -n otel-demo
   ```

2. **Check and fix database connections:**
   ```bash
   # Check database connection pool
   kubectl exec -n otel-demo <pod-name> -- env | grep DB
   
   # Check PostgreSQL connections
   kubectl exec -n otel-demo <postgres-pod> -- psql -U otelu -d otel_demo -c "SELECT count(*) FROM pg_stat_activity;"
   ```

3. **Restart the service:**
   ```bash
   # Restart the deployment
   kubectl rollout restart deployment/<service-name> -n otel-demo
   
   # Monitor rollout status
   kubectl rollout status deployment/<service-name> -n otel-demo
   ```

4. **Check downstream dependencies:**
   ```bash
   # Check if downstream services are healthy
   kubectl get pods -n otel-demo
   kubectl get endpoints -n otel-demo
   ```

5. **Review application logs for errors:**
   ```bash
   # Check for errors in logs
   kubectl logs -n otel-demo -l app.kubernetes.io/name=<service-name> | grep -i error
   ```

### Prevention
- Set up Horizontal Pod Autoscaler (HPA)
- Configure resource requests and limits appropriately
- Implement circuit breakers for downstream calls
- Optimize database queries
- Use caching where appropriate
- Monitor and tune garbage collection settings

---

## High Error Rate

### Alert Details
- **Alert Name**: `HighErrorRate` / `CriticalErrorRate`
- **Severity**: Warning / Critical
- **Threshold**: Error rate > 5% (warning) or > 10% (critical) for 5 minutes
- **Affected**: Services with high 5xx error rates

### What Causes This Alert
- Application bugs or exceptions
- Database connection failures
- Downstream service failures
- Resource exhaustion (OOM, CPU throttling)
- Configuration errors
- Network issues
- Invalid input data

### Diagnosis Steps

1. **Identify error types:**
   ```bash
   # Check pod status
   kubectl get pods -n otel-demo
   
   # View recent errors in logs
   kubectl logs -n otel-demo -l app.kubernetes.io/name=<service-name> --tail=200 | grep -i "error\|exception\|fatal"
   ```

2. **Check HTTP status codes:**
   ```bash
   # Query Prometheus for error breakdown
   # In Prometheus UI:
   # sum(rate(otel_demo_http_requests_total{status_code=~"5..", service_name="<service>"}[5m])) by (status_code)
   ```

3. **Check pod events:**
   ```bash
   # View recent events
   kubectl get events -n otel-demo --sort-by='.lastTimestamp' | tail -20
   
   # Describe pod for detailed events
   kubectl describe pod <pod-name> -n otel-demo
   ```

4. **Check resource constraints:**
   ```bash
   # Check if pods are being OOMKilled
   kubectl get pods -n otel-demo -o json | jq '.items[] | select(.status.containerStatuses[0].lastState.terminated.reason=="OOMKilled")'
   
   # Check resource usage
   kubectl top pods -n otel-demo
   ```

5. **View error traces in Jaeger:**
   ```bash
   # Port-forward to Jaeger
   kubectl port-forward -n otel-demo svc/jaeger-query 16686:16686
   
   # Search for traces with errors (status_code >= 500)
   ```

6. **Check CloudWatch Logs:**
   ```bash
   # Query CloudWatch Logs Insights for errors
   # Use query from observability/cloudwatch/sample-query.md
   # Filter by level = "ERROR" or status_code >= 500
   ```

### Resolution Steps

1. **Restart the service:**
   ```bash
   # Restart deployment
   kubectl rollout restart deployment/<service-name> -n otel-demo
   
   # Monitor rollout
   kubectl rollout status deployment/<service-name> -n otel-demo
   ```

2. **Scale up if resource-constrained:**
   ```bash
   # Increase replicas
   kubectl scale deployment <service-name> --replicas=3 -n otel-demo
   ```

3. **Check and fix configuration:**
   ```bash
   # View configuration
   kubectl get configmap -n otel-demo
   kubectl describe configmap <config-name> -n otel-demo
   
   # Check environment variables
   kubectl get deployment <service-name> -n otel-demo -o yaml | grep -A 10 "env:"
   ```

4. **Fix database connectivity:**
   ```bash
   # Check database pod
   kubectl get pods -n otel-demo -l app=postgres
   
   # Test database connection
   kubectl exec -n otel-demo <service-pod> -- nc -zv <postgres-host> 5432
   ```

5. **Rollback if recent deployment:**
   ```bash
   # Check deployment history
   kubectl rollout history deployment/<service-name> -n otel-demo
   
   # Rollback to previous version
   kubectl rollout undo deployment/<service-name> -n otel-demo
   ```

### Prevention
- Implement proper error handling and retries
- Set up health checks and readiness probes
- Monitor and alert on error rates proactively
- Use feature flags for gradual rollouts
- Implement circuit breakers
- Regular load testing

---

## Pod CrashLoopBackOff

### Alert Details
- **Alert Name**: `PodCrashLoop`
- **Severity**: Critical
- **Threshold**: Pod in CrashLoopBackOff state for > 2 minutes
- **Affected**: Any pod in the otel-demo namespace

### What Causes This Alert
- Application startup failures
- Configuration errors
- Missing dependencies
- Resource constraints (OOM)
- Database connection failures
- Invalid environment variables
- Image pull errors
- Permission issues

### Diagnosis Steps

1. **Identify the crashing pod:**
   ```bash
   # List pods with CrashLoopBackOff
   kubectl get pods -n otel-demo | grep CrashLoopBackOff
   
   # Get detailed pod status
   kubectl describe pod <pod-name> -n otel-demo
   ```

2. **Check pod logs:**
   ```bash
   # View current container logs
   kubectl logs <pod-name> -n otel-demo --tail=100
   
   # View previous container logs (if restarted)
   kubectl logs <pod-name> -n otel-demo --previous --tail=100
   ```

3. **Check events:**
   ```bash
   # View recent events for the pod
   kubectl get events -n otel-demo --field-selector involvedObject.name=<pod-name> --sort-by='.lastTimestamp'
   ```

4. **Check resource limits:**
   ```bash
   # Check if OOMKilled
   kubectl describe pod <pod-name> -n otel-demo | grep -A 5 "Last State"
   
   # Check resource requests/limits
   kubectl get pod <pod-name> -n otel-demo -o json | jq '.spec.containers[0].resources'
   ```

5. **Check configuration:**
   ```bash
   # Check ConfigMaps and Secrets
   kubectl get configmap -n otel-demo
   kubectl get secret -n otel-demo
   
   # Verify environment variables
   kubectl get deployment <deployment-name> -n otel-demo -o yaml | grep -A 20 "env:"
   ```

### Resolution Steps

1. **Fix configuration issues:**
   ```bash
   # If ConfigMap issue, update it
   kubectl edit configmap <config-name> -n otel-demo
   
   # If Secret issue, update it
   kubectl edit secret <secret-name> -n otel-demo
   ```

2. **Increase resource limits if OOM:**
   ```bash
   # Edit deployment to increase memory
   kubectl edit deployment <deployment-name> -n otel-demo
   # Update resources.limits.memory
   ```

3. **Fix application code:**
   ```bash
   # If application error, check logs for stack traces
   kubectl logs <pod-name> -n otel-demo --previous
   
   # Fix the issue and redeploy
   ```

4. **Restart deployment:**
   ```bash
   # After fixing configuration, restart
   kubectl rollout restart deployment/<deployment-name> -n otel-demo
   ```

5. **Delete and recreate pod (last resort):**
   ```bash
   # Delete the pod (deployment will recreate it)
   kubectl delete pod <pod-name> -n otel-demo
   ```

### Prevention
- Proper health checks (liveness and readiness probes)
- Resource requests and limits set appropriately
- Configuration validation on startup
- Graceful error handling
- Comprehensive testing before deployment

---

## Pod Not Ready

### Alert Details
- **Alert Name**: `PodNotReady`
- **Severity**: Warning
- **Threshold**: Pod not ready for > 5 minutes
- **Affected**: Any pod in the otel-demo namespace

### What Causes This Alert
- Readiness probe failures
- Application not responding to health checks
- Slow startup
- Dependency not available
- Resource constraints

### Diagnosis Steps

1. **Check pod status:**
   ```bash
   # List not ready pods
   kubectl get pods -n otel-demo | grep -v Running
   
   # Describe pod
   kubectl describe pod <pod-name> -n otel-demo
   ```

2. **Check readiness probe:**
   ```bash
   # View readiness probe configuration
   kubectl get pod <pod-name> -n otel-demo -o yaml | grep -A 10 "readinessProbe"
   
   # Test readiness endpoint manually
   kubectl exec <pod-name> -n otel-demo -- curl -f http://localhost:<port>/health || echo "Failed"
   ```

3. **Check logs:**
   ```bash
   # View application logs
   kubectl logs <pod-name> -n otel-demo --tail=100
   ```

### Resolution Steps

1. **Fix readiness probe endpoint:**
   ```bash
   # If endpoint is wrong, update deployment
   kubectl edit deployment <deployment-name> -n otel-demo
   ```

2. **Increase readiness probe timeout:**
   ```bash
   # If startup is slow, increase initialDelaySeconds
   kubectl edit deployment <deployment-name> -n otel-demo
   ```

3. **Fix application health check:**
   ```bash
   # Ensure health endpoint is working
   kubectl exec <pod-name> -n otel-demo -- curl http://localhost:<port>/health
   ```

---

## High Memory Usage

### Alert Details
- **Alert Name**: `HighMemoryUsage`
- **Severity**: Warning
- **Threshold**: Memory usage > 90% of limit for 5 minutes
- **Affected**: Pods in otel-demo namespace

### Diagnosis Steps

1. **Check memory usage:**
   ```bash
   kubectl top pods -n otel-demo
   ```

2. **Check memory limits:**
   ```bash
   kubectl describe pod <pod-name> -n otel-demo | grep -A 5 "Limits"
   ```

### Resolution Steps

1. **Increase memory limits:**
   ```bash
   kubectl edit deployment <deployment-name> -n otel-demo
   # Update resources.limits.memory
   ```

2. **Scale horizontally:**
   ```bash
   kubectl scale deployment <deployment-name> --replicas=3 -n otel-demo
   ```

3. **Investigate memory leaks:**
   ```bash
   # Check for memory leaks in logs
   kubectl logs <pod-name> -n otel-demo | grep -i "memory\|oom\|gc"
   ```

---

## High CPU Usage

### Alert Details
- **Alert Name**: `HighCPUUsage`
- **Severity**: Warning
- **Threshold**: CPU usage > 90% of limit for 5 minutes

### Resolution Steps

1. **Scale horizontally:**
   ```bash
   kubectl scale deployment <deployment-name> --replicas=3 -n otel-demo
   ```

2. **Increase CPU limits:**
   ```bash
   kubectl edit deployment <deployment-name> -n otel-demo
   ```

---

## OpenTelemetry Collector Down

### Alert Details
- **Alert Name**: `OtelCollectorDown`
- **Severity**: Critical
- **Threshold**: Collector down for > 1 minute

### Resolution Steps

1. **Check collector pod:**
   ```bash
   kubectl get pods -n otel-demo -l app.kubernetes.io/name=otelcol
   kubectl describe pod <collector-pod> -n otel-demo
   kubectl logs <collector-pod> -n otel-demo
   ```

2. **Restart collector:**
   ```bash
   kubectl rollout restart deployment/otelcol -n otel-demo
   ```

---

## OpenTelemetry Collector High Error Rate

### Alert Details
- **Alert Name**: `OtelCollectorHighErrorRate`
- **Severity**: Warning
- **Threshold**: > 10 failed spans per second for 5 minutes

### Resolution Steps

1. **Check collector configuration:**
   ```bash
   kubectl get configmap -n otel-demo | grep otel
   kubectl describe configmap <otel-config> -n otel-demo
   ```

2. **Check exporter endpoints:**
   ```bash
   # Verify exporter endpoints are reachable
   kubectl exec <collector-pod> -n otel-demo -- curl -f <exporter-endpoint>
   ```

---

## Service Down

### Alert Details
- **Alert Name**: `ServiceDown`
- **Severity**: Critical
- **Threshold**: Service unreachable for > 2 minutes

### Resolution Steps

1. **Check service endpoints:**
   ```bash
   kubectl get endpoints -n otel-demo
   kubectl describe service <service-name> -n otel-demo
   ```

2. **Check pods:**
   ```bash
   kubectl get pods -n otel-demo -l app.kubernetes.io/name=<service-name>
   ```

3. **Restart service:**
   ```bash
   kubectl rollout restart deployment/<service-name> -n otel-demo
   ```

---

## General Troubleshooting Commands

```bash
# View all resources in namespace
kubectl get all -n otel-demo

# Check cluster events
kubectl get events -n otel-demo --sort-by='.lastTimestamp'

# View service mesh status (if using Istio/Linkerd)
kubectl get virtualservice -n otel-demo
kubectl get destinationrule -n otel-demo

# Check network policies
kubectl get networkpolicy -n otel-demo

# View resource quotas
kubectl describe quota -n otel-demo
```

---

## Escalation

If the issue cannot be resolved using this runbook:

1. Check the OpenTelemetry Demo GitHub issues: https://github.com/open-telemetry/opentelemetry-demo/issues
2. Review application-specific documentation
3. Contact the development team
4. Review recent deployments and changes



